{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> York University - ML1030 - Julia Mitroi </div>\n",
    "\n",
    "# Sentiment Analysis for Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing/Normalization + Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = pd.read_csv(\"train.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = pd.read_csv(\"test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reviews text lowercase\n",
    "train_reviews['Phrase'] = train_reviews['Phrase'].str.lower()\n",
    "test_reviews['Phrase'] = test_reviews['Phrase'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving only letters from a-z and digits\n",
    "train_reviews['Phrase'] = train_reviews['Phrase'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "test_reviews['Phrase'] = test_reviews['Phrase'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing HTML content\n",
    "train_reviews['Phrase'] = [BeautifulSoup(text).get_text() for text in train_reviews['Phrase']]\n",
    "test_reviews['Phrase'] = [BeautifulSoup(text).get_text() for text in test_reviews['Phrase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization using the TweetTokenizer, a Twitter-aware tokenizer which was designed to be flexible and easy to adapt to new domains and tasks\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "def tokenize(t):\n",
    "    return ' '.join(tt.tokenize(t))\n",
    "\n",
    "sentences = train_reviews.Phrase.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(l):\n",
    "    return ' '.join([lemmatizer.lemmatize(s) for s in l.split(' ')])    \n",
    "    \n",
    "sentences = sentences.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's Stemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(s):\n",
    "    return ' '.join([stemmer.stem(w) for w in s.split(' ')])\n",
    "\n",
    "sentences = sentences.apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary to modeling, I will use Keras text preprocessing functions to further pre-process the sentences in the \n",
    "# train set, namely to transform the text into sequences of tokens, and pad those sequences to have the same length.\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "# fit_on_texts function creates vocabulary index based on word frequency in sentences\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# texts_to_sequences transforms each text in the sentences set to a sequence of integers\n",
    "X = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# pad_sequences performs sequences padding\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 48)\n"
     ]
    }
   ],
   "source": [
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using k-Fold Cross-Validation, splitting the train_reviews the dataset into 10 groups, shuffling the dataset randomly\n",
    "kf = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the multinomina Naive Bayes model\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X, train_reviews['Sentiment']):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = train_reviews['Sentiment'][train_index], train_reviews['Sentiment'][test_index]\n",
    "    mnb = mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy classification score: \n",
      "0.4637959759067026\n"
     ]
    }
   ],
   "source": [
    "print(\"average accuracy classification score: \")\n",
    "print (accuracy_score(y_test, mnb.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
